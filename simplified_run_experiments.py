#!/usr/bin/env python3
# simplified_run_experiments.py - å®Œæ•´é›†æˆå®éªŒï¼ˆQ/Rç¼©æ”¾ä¸“ç”¨ç‰ˆï¼‰
"""
é›†æˆ7ç§é«˜è´¨é‡åŸºçº¿æ–¹æ³•å¯¹æ¯”å®éªŒï¼š
- 5ç§æ ¸å¿ƒæ–¹æ³•ï¼ˆå›ºå®šUKFã€ç‰©ç†æ–¹æ³•ã€Q/Rç¼©æ”¾UKFã€Kalman-RNNã€æ™ºèƒ½UKFï¼‰
- VECTORæ–¹æ³•ï¼ˆä½¿ç”¨è®­ç»ƒå¥½çš„vector_best_model.pthï¼‰
- å¾®åˆ†å¹³å¦æ€§ç‰©ç†é¢„æµ‹å™¨ï¼ˆé’ˆå¯¹4ç§è½½å…·ç±»å‹ï¼‰
"""

import argparse
import sys
import os
import numpy as np
import pandas as pd
import warnings
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import time
from typing import Optional, Dict, Any, List, Tuple
from scipy.optimize import minimize, least_squares

warnings.filterwarnings("ignore", category=UserWarning)

# å¯¼å…¥æ ¸å¿ƒåŸºçº¿æ–¹æ³•
sys.path.append('.')
try:
    from final_working_baseline_methods import create_baseline_methods, MethodWrapper

    print("âœ… æˆåŠŸå¯¼å…¥æ ¸å¿ƒåŸºçº¿æ–¹æ³•")
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ ¸å¿ƒåŸºçº¿æ–¹æ³•å¤±è´¥: {e}")
    sys.exit(1)

# å¯¼å…¥æ–°è½¨è¿¹é¢„æµ‹æ–¹æ³•ï¼ˆä»…ç”¨äºTrajectron++ï¼‰
try:
    from new_trajectory_methods import (
        create_optimized_trajectron_plus_plus,
        get_available_methods,
    )

    NEW_METHODS_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥Trajectron++ç›¸å…³æ–¹æ³•")
except ImportError as e:
    NEW_METHODS_AVAILABLE = False
    print(f"âš ï¸ Trajectron++æ–¹æ³•å¯¼å…¥å¤±è´¥: {e}")
    print("   å°†åªä½¿ç”¨æ ¸å¿ƒæ–¹æ³•ã€VECTORå’Œç‰©ç†æ–¹æ³•")
# åœ¨ç°æœ‰å¯¼å…¥ä¹‹åæ·»åŠ 
try:
    from final_working_baseline_methods import (
        QRScalingTransformerNN,
        LightweightReservoir,
        SmartQREnhancedUKF
    )
    print("âœ… æˆåŠŸå¯¼å…¥Q/Rç¼©æ”¾ç›¸å…³ç±»å®šä¹‰")
except ImportError as e:
    print(f"âš ï¸ Q/Rç¼©æ”¾ç±»å¯¼å…¥å¤±è´¥: {e}")

# =========================
# å¾®åˆ†å¹³å¦æ€§ç‰©ç†é¢„æµ‹å™¨
# =========================
import numpy as np
from collections import defaultdict


def print_final_results_with_stats(all_runs_results, num_runs=5):
    """è¾“å‡ºå¤šæ¬¡è¿è¡Œçš„ç»Ÿè®¡ç»“æœï¼ˆå‡å€¼Â±æ ‡å‡†å·®ï¼‰"""
    print(f"\n{'=' * 60}")
    print(f"è½¨è¿¹é¢„æµ‹æ–¹æ³•å¯¹æ¯”ç»“æœ (åŸºäº{num_runs}æ¬¡ç‹¬ç«‹è¿è¡Œ)")
    print("=" * 60)

    # æ”¶é›†æ‰€æœ‰æ–¹æ³•åœ¨æ‰€æœ‰è¿è¡Œä¸­çš„æŒ‡æ ‡
    method_stats = defaultdict(lambda: {
        'ADE': [], 'FDE': [], 'Success_Rate': [], 'Processing_Time': []
    })

    # éå†æ‰€æœ‰è¿è¡Œç»“æœ
    for run_results in all_runs_results:
        for result in run_results:
            for method_name, metrics in result.items():
                if 'dataset_info' in method_name:
                    continue

                if metrics['ADE'] != float('inf'):
                    method_stats[method_name]['ADE'].append(metrics['ADE'])
                    method_stats[method_name]['FDE'].append(metrics['FDE'])
                    method_stats[method_name]['Success_Rate'].append(metrics['Success_Rate'])
                    method_stats[method_name]['Processing_Time'].append(metrics['Processing_Time'])

    # è®¡ç®—æ¯ä¸ªæ–¹æ³•çš„ç»Ÿè®¡é‡
    final_stats = []
    for method_name, stats in method_stats.items():
        if len(stats['ADE']) > 0:
            ade_mean = np.mean(stats['ADE'])
            ade_std = np.std(stats['ADE'])
            fde_mean = np.mean(stats['FDE'])
            fde_std = np.std(stats['FDE'])
            success_mean = np.mean(stats['Success_Rate'])
            success_std = np.std(stats['Success_Rate'])
            time_mean = np.mean(stats['Processing_Time'])
            time_std = np.std(stats['Processing_Time'])

            final_stats.append((
                method_name,
                ade_mean, ade_std,
                fde_mean, fde_std,
                success_mean, success_std,
                time_mean, time_std,
                len(stats['ADE'])  # æœ‰æ•ˆè¿è¡Œæ¬¡æ•°
            ))

    # æŒ‰ADEå‡å€¼æ’åº
    final_stats.sort(key=lambda x: x[1])

    print("\næ€»ä½“æ€§èƒ½æ’å (å‡å€¼Â±æ ‡å‡†å·®):")
    print("-" * 80)
    for rank, (method_name, ade_mean, ade_std, fde_mean, fde_std,
               success_mean, success_std, time_mean, time_std, valid_runs) in enumerate(final_stats, 1):
        method_type = "ğŸ§ " if any(x in method_name for x in ['Q/R', 'VECTOR', 'Transformer', 'Trajectron']) else "âš™ï¸"

        print(f"#{rank} {method_type} {method_name} ({valid_runs}/{num_runs} runs):")
        print(f"   ADE: {ade_mean:.6f}Â±{ade_std:.6f}m")
        print(f"   FDE: {fde_mean:.6f}Â±{fde_std:.6f}m")
        print(f"   Success Rate: {success_mean:.1f}Â±{success_std:.1f}%")
        print(f"   Processing Time: {time_mean:.3f}Â±{time_std:.3f}ms")
        print()

    # è®ºæ–‡æ ¼å¼è¾“å‡º
    print("è®ºæ–‡è¡¨æ ¼æ ¼å¼:")
    print("-" * 80)
    print("Method | ADE (m) | FDE (m) | Success Rate (%) | Time (ms)")
    print("-" * 80)
    for method_name, ade_mean, ade_std, fde_mean, fde_std, success_mean, success_std, time_mean, time_std, valid_runs in final_stats:
        print(
            f"{method_name} | {ade_mean:.6f}Â±{ade_std:.6f} | {fde_mean:.6f}Â±{fde_std:.6f} | {success_mean:.1f}Â±{success_std:.1f} | {time_mean:.3f}Â±{time_std:.3f}")

    # LaTeXè¡¨æ ¼æ ¼å¼
    print("\nLaTeXè¡¨æ ¼æ ¼å¼:")
    print("-" * 80)
    for method_name, ade_mean, ade_std, fde_mean, fde_std, success_mean, success_std, time_mean, time_std, valid_runs in final_stats:
        latex_name = method_name.replace('_', '\\_')
        print(
            f"{latex_name} & ${ade_mean:.6f} \\pm {ade_std:.6f}$ & ${fde_mean:.6f} \\pm {fde_std:.6f}$ & ${success_mean:.1f} \\pm {success_std:.1f}$ & ${time_mean:.3f} \\pm {time_std:.3f}$ \\\\")

    # Q/Rç¼©æ”¾æ–¹æ³•ç‰¹åˆ«åˆ†æ
    print(f"\n{'=' * 60}")
    print("Q/Rç¼©æ”¾æ–¹æ³•ç»Ÿè®¡åˆ†æ:")
    print("=" * 60)

    qr_found = False
    for method_name, ade_mean, ade_std, fde_mean, fde_std, success_mean, success_std, time_mean, time_std, valid_runs in final_stats:
        if 'Q/Rç¼©æ”¾' in method_name:
            rank = [x[0] for x in final_stats].index(method_name) + 1
            print(f"ğŸ¯ {method_name}:")
            print(f"   æ’å: #{rank}/{len(final_stats)}")
            print(f"   ADEç¨³å®šæ€§: {ade_mean:.6f}Â±{ade_std:.6f}m (å˜å¼‚ç³»æ•°: {ade_std / ade_mean * 100:.2f}%)")
            print(f"   FDEç¨³å®šæ€§: {fde_mean:.6f}Â±{fde_std:.6f}m (å˜å¼‚ç³»æ•°: {fde_std / fde_mean * 100:.2f}%)")
            print(
                f"   æˆåŠŸç‡ç¨³å®šæ€§: {success_mean:.1f}Â±{success_std:.1f}% (å˜å¼‚ç³»æ•°: {success_std / success_mean * 100:.2f}%)")
            print(f"   è®¡ç®—æ•ˆç‡: {time_mean:.3f}Â±{time_std:.3f}ms")
            print(f"   æœ‰æ•ˆè¿è¡Œ: {valid_runs}/{num_runs}")

            # æ€§èƒ½è¯„ä»·
            cv_ade = ade_std / ade_mean * 100
            performance_level = "ä¼˜ç§€" if rank <= 2 else "è‰¯å¥½" if rank <= 4 else "ä¸­ç­‰"
            stability_level = "é«˜" if cv_ade < 10 else "ä¸­" if cv_ade < 20 else "ä½"

            print(f"   æ€§èƒ½è¯„ä»·: {performance_level}")
            print(f"   ç¨³å®šæ€§: {stability_level} (ADEå˜å¼‚ç³»æ•° {cv_ade:.2f}%)")
            print(f"   å®æ—¶æ€§: {'æ»¡è¶³' if time_mean < 10.0 else 'ä¸æ»¡è¶³'} (<10msè¦æ±‚)")
            qr_found = True
            break

    if not qr_found:
        print("âŒ Q/Rç¼©æ”¾æ–¹æ³•æœªå‚ä¸è¯„ä¼°")

    return final_stats
class DifferentialFlatnessTrajectoryPredictor:
    """åŸºäºå¾®åˆ†å¹³å¦æ€§çš„UAVè½¨è¿¹é¢„æµ‹å™¨"""

    def __init__(self, dt=0.1, vehicle_type="medium_large_quad"):
        self.dt = dt
        self.vehicle_type = vehicle_type

        # è½½å…·ç‰©ç†å‚æ•°
        self.params = self._get_vehicle_parameters(vehicle_type)

        # çŠ¶æ€å†å²ç¼“å†²åŒº
        self.position_history = deque(maxlen=15)
        self.velocity_history = deque(maxlen=10)
        self.acceleration_history = deque(maxlen=8)

        # å¾®åˆ†å¹³å¦æ€§ç›¸å…³
        self.polynomial_order = 7  # ç”¨äºè½¨è¿¹æ‹Ÿåˆçš„å¤šé¡¹å¼é˜¶æ•°
        self.optimization_horizon = 5  # ä¼˜åŒ–æ—¶åŸŸæ­¥æ•°

        # çº¦æŸå‚æ•°
        self.constraints = self._get_dynamic_constraints()

        # å‚æ•°ä¼°è®¡
        self.mass_estimator = AdaptiveMassEstimator(self.params['nominal_mass'])
        self.drag_estimator = DragCoefficientEstimator()

        # ç»Ÿè®¡ä¿¡æ¯
        self.prediction_count = 0
        self.constraint_violations = 0
        self.optimization_failures = 0

        self.initialized = False

    def _get_vehicle_parameters(self, vehicle_type: str) -> Dict:
        """è·å–è½½å…·ç‰¹å®šçš„ç‰©ç†å‚æ•°"""
        params_db = {
            "micro_quad": {
                "nominal_mass": 0.20,  # kg
                "max_thrust": 51.9,  # N
                "max_tilt_angle": np.deg2rad(60),
                "max_angular_velocity": np.deg2rad(600),
                "drag_coefficient": 0.2351,
                "moment_arm": 0.1,  # m
                "base_max_snap": 200.0,  # m/sâ´
            },
            "medium_large_quad": {
                "nominal_mass": 1.50,  # kg
                "max_thrust": 79.2,  # N
                "max_tilt_angle": np.deg2rad(50),
                "max_angular_velocity": np.deg2rad(140),
                "drag_coefficient": 0.0182,
                "moment_arm": 0.25,  # m
                "base_max_snap": 760.4,  # m/sâ´
            },
            "fixed_wing": {
                "nominal_mass": 2.60,  # kg
                "max_thrust": 50.0,  # N
                "max_bank_angle": np.deg2rad(60),
                "max_load_factor": 3.0,
                "min_velocity": 16.2,  # m/s
                "drag_coefficient": 0.0250,
                "lift_coefficient": 0.8,
                "wing_area": 0.5,  # m^2
                "base_max_snap": 10.1,  # m/sâ´
            },
            "heavy_multirotor": {
                "nominal_mass": 4.00,  # kg
                "max_thrust": 251.0,  # N
                "max_tilt_angle": np.deg2rad(45),
                "max_angular_velocity": np.deg2rad(173.9),
                "drag_coefficient": 0.0569,
                "moment_arm": 0.4,  # m
                "base_max_snap": 1856.4,  # m/sâ´
            }
        }

        return params_db.get(vehicle_type, params_db["medium_large_quad"])

    def _get_dynamic_constraints(self) -> Dict:
        """è·å–åŠ¨åŠ›å­¦çº¦æŸ"""
        if self.vehicle_type == "fixed_wing":
            return {
                "max_acceleration": 17.3,  # m/sÂ²
                "max_velocity": 42.1,  # m/s
                "min_velocity": 16.2,  # m/s
                "max_climb_rate": 7.2,  # m/s
                "max_turn_rate": np.deg2rad(20.0),  # rad/s
            }
        else:  # å¤šæ—‹ç¿¼ç±»å‹
            constraints_db = {
                "micro_quad": {
                    "max_acceleration": min(227.1, 100.0),  # m/sÂ²
                    "max_velocity": 32.4,  # m/s
                    "max_vertical_velocity": 11.4,  # m/s
                    "max_tilt_angle": np.deg2rad(60),
                    "max_angular_velocity": np.deg2rad(600),
                },
                "medium_large_quad": {
                    "max_acceleration": 38.7,  # m/sÂ²
                    "max_velocity": 35.1,  # m/s
                    "max_vertical_velocity": 17.3,  # m/s
                    "max_tilt_angle": np.deg2rad(50),
                    "max_angular_velocity": np.deg2rad(140),
                },
                "heavy_multirotor": {
                    "max_acceleration": 47.6,  # m/sÂ²
                    "max_velocity": 24.2,  # m/s
                    "max_vertical_velocity": 7.2,  # m/s
                    "max_tilt_angle": np.deg2rad(45),
                    "max_angular_velocity": np.deg2rad(173.9),
                }
            }
            return constraints_db.get(self.vehicle_type, constraints_db["medium_large_quad"])

    def _get_max_snap(self) -> float:
        """è·å–æœ€å¤§å…è®¸snapå€¼"""
        snap_db = {
            "micro_quad": 200.0,  # m/s^4
            "medium_large_quad": 760.4,  # m/s^4
            "fixed_wing": 10.1,  # m/s^4
            "heavy_multirotor": 1856.4  # m/s^4
        }
        return snap_db.get(self.vehicle_type, 30.0)

    def initialize(self, initial_state: np.ndarray) -> bool:
        """åˆå§‹åŒ–é¢„æµ‹å™¨"""
        try:
            self.position_history.clear()
            self.velocity_history.clear()
            self.acceleration_history.clear()

            # åˆå§‹åŒ–çŠ¶æ€
            initial_pos = initial_state[:3]
            self.position_history.append(initial_pos.copy())
            self.velocity_history.append(np.zeros(3))
            self.acceleration_history.append(np.zeros(3))

            # é‡ç½®ä¼°è®¡å™¨
            self.mass_estimator.reset(self.params['nominal_mass'])
            self.drag_estimator.reset()

            # é‡ç½®ç»Ÿè®¡
            self.prediction_count = 0
            self.constraint_violations = 0
            self.optimization_failures = 0

            self.initialized = True
            return True

        except Exception as e:
            print(f"å¾®åˆ†å¹³å¦æ€§é¢„æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def predict_and_update(self, measurement: np.ndarray, **kwargs) -> np.ndarray:
        """åŸºäºå¾®åˆ†å¹³å¦æ€§çš„è½¨è¿¹é¢„æµ‹"""
        self.prediction_count += 1
        current_pos = measurement[:3]

        if not self.initialized:
            return current_pos

        try:
            # æ›´æ–°çŠ¶æ€å†å²
            self.position_history.append(current_pos.copy())

            # è®¡ç®—é€Ÿåº¦å’ŒåŠ é€Ÿåº¦
            if len(self.position_history) >= 2:
                velocity = self._estimate_velocity()
                self.velocity_history.append(velocity.copy())

                if len(self.velocity_history) >= 2:
                    acceleration = self._estimate_acceleration()
                    self.acceleration_history.append(acceleration.copy())

            # æ‰§è¡Œå¾®åˆ†å¹³å¦æ€§é¢„æµ‹
            if len(self.position_history) >= 5:
                predicted_pos = self._differential_flatness_prediction()

                # éªŒè¯åŠ¨åŠ›å­¦å¯è¡Œæ€§
                if self._validate_dynamics(predicted_pos):
                    return predicted_pos
                else:
                    self.constraint_violations += 1
                    return self._constrained_prediction()
            else:
                # æ•°æ®ä¸è¶³æ—¶ä½¿ç”¨ç®€å•å¤–æ¨
                return self._simple_extrapolation()

        except Exception as e:
            self.optimization_failures += 1
            if self.prediction_count <= 3:
                print(f"å¾®åˆ†å¹³å¦æ€§é¢„æµ‹å¤±è´¥: {e}")
            return self._fallback_prediction(current_pos)

    def _estimate_velocity(self) -> np.ndarray:
        """ä¼°è®¡å½“å‰é€Ÿåº¦"""
        if len(self.position_history) >= 3:
            # ä½¿ç”¨3ç‚¹æ•°å€¼å¾®åˆ†
            positions = np.array(list(self.position_history)[-3:])
            velocity = (-3 * positions[0] + 4 * positions[1] - positions[2]) / (2 * self.dt)
        else:
            positions = np.array(list(self.position_history)[-2:])
            velocity = (positions[1] - positions[0]) / self.dt

        return velocity

    def _estimate_acceleration(self) -> np.ndarray:
        """ä¼°è®¡å½“å‰åŠ é€Ÿåº¦"""
        if len(self.velocity_history) >= 3:
            velocities = np.array(list(self.velocity_history)[-3:])
            acceleration = (-3 * velocities[0] + 4 * velocities[1] - velocities[2]) / (2 * self.dt)
        else:
            velocities = np.array(list(self.velocity_history)[-2:])
            acceleration = (velocities[1] - velocities[0]) / self.dt

        return acceleration

    def _differential_flatness_prediction(self) -> np.ndarray:
        """æ ¸å¿ƒï¼šåŸºäºå¾®åˆ†å¹³å¦æ€§çš„è½¨è¿¹é¢„æµ‹"""
        # è·å–å†å²è½¨è¿¹ç‚¹
        positions = np.array(list(self.position_history)[-8:])  # ä½¿ç”¨æœ€è¿‘8ä¸ªç‚¹
        t_history = np.arange(len(positions)) * (-self.dt)  # æ—¶é—´å‘é‡ï¼ˆè´Ÿæ•°è¡¨ç¤ºè¿‡å»ï¼‰

        # ä¸ºæ¯ä¸ªåæ ‡è½´æ‹Ÿåˆå¤šé¡¹å¼
        predicted_pos = np.zeros(3)

        for axis in range(3):
            # æ‹Ÿåˆ7é˜¶å¤šé¡¹å¼ï¼ˆç¡®ä¿snapè¿ç»­æ€§ï¼‰
            try:
                coeffs = np.polyfit(t_history, positions[:, axis], min(self.polynomial_order, len(positions) - 1))
                poly = np.poly1d(coeffs)

                # é¢„æµ‹ä¸‹ä¸€æ—¶åˆ» (t = dt)
                predicted_pos[axis] = poly(self.dt)

                # æ£€æŸ¥snapï¼ˆå››é˜¶å¯¼æ•°ï¼‰çº¦æŸ
                snap = self._compute_polynomial_snap(coeffs, self.dt)
                if abs(snap) > self._get_max_snap():
                    # Snapè¿‡å¤§ï¼Œä½¿ç”¨çº¦æŸä¼˜åŒ–
                    predicted_pos[axis] = self._constrained_polynomial_prediction(
                        t_history, positions[:, axis], axis
                    )

            except (np.linalg.LinAlgError, np.RankWarning):
                # å¤šé¡¹å¼æ‹Ÿåˆå¤±è´¥ï¼Œé™é˜¶é‡è¯•
                try:
                    coeffs = np.polyfit(t_history, positions[:, axis], 3)
                    poly = np.poly1d(coeffs)
                    predicted_pos[axis] = poly(self.dt)
                except:
                    # å†æ¬¡å¤±è´¥ï¼Œä½¿ç”¨çº¿æ€§å¤–æ¨
                    predicted_pos[axis] = positions[-1, axis] + (positions[-1, axis] - positions[-2, axis])

        return predicted_pos

    def _compute_polynomial_snap(self, coeffs: np.ndarray, t: float) -> float:
        """è®¡ç®—å¤šé¡¹å¼çš„snapï¼ˆå››é˜¶å¯¼æ•°ï¼‰"""
        if len(coeffs) < 5:
            return 0.0

        # å››é˜¶å¯¼æ•°çš„ç³»æ•°
        snap_coeffs = []
        for i in range(len(coeffs) - 4):
            coeff = coeffs[i]
            for j in range(4):
                coeff *= (len(coeffs) - 1 - i - j)
            snap_coeffs.append(coeff)

        if len(snap_coeffs) == 0:
            return 0.0

        # åœ¨æ—¶åˆ»tè®¡ç®—snap
        snap = 0.0
        for i, coeff in enumerate(snap_coeffs):
            snap += coeff * (t ** (len(snap_coeffs) - 1 - i))

        return snap

    def _constrained_polynomial_prediction(self, t_history: np.ndarray,
                                           position_history: np.ndarray, axis: int) -> float:
        """çº¦æŸä¼˜åŒ–çš„å¤šé¡¹å¼é¢„æµ‹"""
        try:
            # ç®€åŒ–çš„çº¦æŸé¢„æµ‹ï¼šä½¿ç”¨ä½é˜¶å¤šé¡¹å¼
            coeffs = np.polyfit(t_history, position_history, 3)
            poly = np.poly1d(coeffs)
            return poly(self.dt)
        except:
            # å¼‚å¸¸æƒ…å†µï¼Œä½¿ç”¨çº¿æ€§å¤–æ¨
            return position_history[-1] + (position_history[-1] - position_history[-2])

    def _validate_dynamics(self, predicted_pos: np.ndarray) -> bool:
        """éªŒè¯é¢„æµ‹ç»“æœçš„åŠ¨åŠ›å­¦å¯è¡Œæ€§"""
        current_pos = np.array(list(self.position_history)[-1])

        # æ£€æŸ¥é€Ÿåº¦çº¦æŸ
        predicted_velocity = (predicted_pos - current_pos) / self.dt
        velocity_magnitude = np.linalg.norm(predicted_velocity)

        if velocity_magnitude > self.constraints['max_velocity']:
            return False

        # å¯¹äºå›ºå®šç¿¼ï¼Œæ£€æŸ¥æœ€å°é€Ÿåº¦
        if self.vehicle_type == "fixed_wing":
            if velocity_magnitude < self.constraints['min_velocity']:
                return False

        # æ£€æŸ¥åŠ é€Ÿåº¦çº¦æŸ
        if len(self.velocity_history) > 0:
            current_velocity = self.velocity_history[-1]
            predicted_acceleration = (predicted_velocity - current_velocity) / self.dt
            accel_magnitude = np.linalg.norm(predicted_acceleration)

            if accel_magnitude > self.constraints['max_acceleration']:
                return False

        return True

    def _constrained_prediction(self) -> np.ndarray:
        """çº¦æŸä¼˜åŒ–çš„é¢„æµ‹"""
        current_pos = np.array(list(self.position_history)[-1])
        current_vel = self.velocity_history[-1] if self.velocity_history else np.zeros(3)

        # ç®€å•çš„çº¦æŸå¤„ç†ï¼šé™åˆ¶é€Ÿåº¦
        simple_pred = self._simple_extrapolation()
        pred_velocity = (simple_pred - current_pos) / self.dt

        # é™åˆ¶é€Ÿåº¦
        velocity_magnitude = np.linalg.norm(pred_velocity)
        if velocity_magnitude > self.constraints['max_velocity']:
            pred_velocity = pred_velocity * (self.constraints['max_velocity'] / velocity_magnitude)

        return current_pos + pred_velocity * self.dt

    def _simple_extrapolation(self) -> np.ndarray:
        """ç®€å•çº¿æ€§å¤–æ¨"""
        if len(self.position_history) >= 2:
            positions = np.array(list(self.position_history)[-2:])
            velocity = (positions[1] - positions[0]) / self.dt

            # é€Ÿåº¦é™åˆ¶
            velocity_magnitude = np.linalg.norm(velocity)
            if velocity_magnitude > self.constraints['max_velocity']:
                velocity = velocity * (self.constraints['max_velocity'] / velocity_magnitude)

            return positions[1] + velocity * self.dt
        else:
            return np.array(list(self.position_history)[-1])

    def _fallback_prediction(self, current_pos: np.ndarray) -> np.ndarray:
        """åº”æ€¥é¢„æµ‹æ–¹æ¡ˆ"""
        if len(self.position_history) >= 2:
            prev_pos = np.array(list(self.position_history)[-2])
            safe_velocity = (current_pos - prev_pos) / self.dt

            # ä¿å®ˆçš„é€Ÿåº¦é™åˆ¶
            max_safe_velocity = self.constraints['max_velocity'] * 0.5
            velocity_magnitude = np.linalg.norm(safe_velocity)
            if velocity_magnitude > max_safe_velocity:
                safe_velocity = safe_velocity * (max_safe_velocity / velocity_magnitude)

            return current_pos + safe_velocity * self.dt
        else:
            return current_pos

    def get_debug_info(self) -> Dict:
        """è·å–è°ƒè¯•ä¿¡æ¯"""
        success_rate = ((self.prediction_count - self.optimization_failures) /
                        max(self.prediction_count, 1)) * 100

        return {
            'method': 'Differential Flatness Physics',
            'vehicle_type': self.vehicle_type,
            'predictions': self.prediction_count,
            'success_rate': f"{success_rate:.1f}%",
            'constraint_violations': self.constraint_violations,
            'optimization_failures': self.optimization_failures,
            'current_mass_estimate': f"{self.mass_estimator.get_current_estimate():.2f}kg",
            'polynomial_order': self.polynomial_order
        }


class AdaptiveMassEstimator:
    """è‡ªé€‚åº”è´¨é‡ä¼°è®¡å™¨"""

    def __init__(self, initial_mass: float):
        self.initial_mass = initial_mass
        self.current_estimate = initial_mass
        self.measurements = deque(maxlen=20)

    def reset(self, mass: float):
        self.current_estimate = mass
        self.measurements.clear()

    def get_current_estimate(self) -> float:
        return self.current_estimate


class DragCoefficientEstimator:
    """é˜»åŠ›ç³»æ•°ä¼°è®¡å™¨"""

    def __init__(self):
        self.drag_estimates = deque(maxlen=15)
        self.current_drag = 0.02

    def reset(self):
        self.drag_estimates.clear()
        self.current_drag = 0.02

    def get_current_drag(self) -> float:
        return self.current_drag


# =========================
# VECTORç›¸å…³ç±»å®šä¹‰
# =========================

class VectorGRU(nn.Module):
    """VECTOR GRUæ¨¡å‹"""

    def __init__(self, input_dim=3, hidden_dim=64, num_layers=2,
                 output_dim=3, dropout=0.5, sequence_length=20):
        super(VectorGRU, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.sequence_length = sequence_length

        # GRUå±‚å¤„ç†é€Ÿåº¦åºåˆ—
        self.gru = nn.GRU(
            input_dim, hidden_dim, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0
        )

        # è¾“å‡ºå±‚ï¼šä»GRUéšçŠ¶æ€åˆ°ä½ç½®é¢„æµ‹
        self.position_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, output_dim)
        )

        # é€Ÿåº¦é¢„æµ‹åˆ†æ”¯
        self.velocity_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim),
            nn.Tanh()
        )

    def forward(self, velocity_sequence, last_position=None):
        """å‰å‘ä¼ æ’­"""
        # GRUå¤„ç†é€Ÿåº¦åºåˆ—
        gru_out, hidden = self.gru(velocity_sequence)

        # ä½¿ç”¨æœ€åæ—¶åˆ»çš„éšçŠ¶æ€
        last_hidden = gru_out[:, -1, :]

        # é¢„æµ‹ä¸‹ä¸€æ­¥é€Ÿåº¦
        predicted_velocity = self.velocity_predictor(last_hidden)

        # å¦‚æœæä¾›äº†æœ€åä½ç½®ï¼Œé€šè¿‡ç§¯åˆ†å¾—åˆ°ä¸‹ä¸€æ­¥ä½ç½®
        if last_position is not None:
            predicted_position = last_position + predicted_velocity * 0.1  # dt=0.1
        else:
            position_delta = self.position_predictor(last_hidden)
            predicted_position = position_delta

        return predicted_position, predicted_velocity


class VectorPredictor:
    """VECTORé¢„æµ‹å™¨"""

    def __init__(self, model_path=None, sequence_length=20, dt=0.1, device='cpu'):
        self.sequence_length = sequence_length
        self.dt = dt
        self.device = device

        # é€Ÿåº¦å†å²ç¼“å†²åŒº
        self.velocity_buffer = deque(maxlen=sequence_length)
        self.position_history = deque(maxlen=10)

        # æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
        self.model = None
        self.position_scaler = None
        self.velocity_scaler = None
        self.model_loaded = False

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_predictions = 0
        self.successful_predictions = 0

        if model_path and os.path.exists(model_path):
            self.load_model(model_path)

    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„VECTORæ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)

            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            self.model = VectorGRU(
                input_dim=3,
                hidden_dim=checkpoint.get('hidden_dim', 64),
                num_layers=checkpoint.get('num_layers', 2),
                output_dim=3,
                dropout=0.0,  # æ¨ç†æ—¶ä¸ä½¿ç”¨dropout
                sequence_length=self.sequence_length
            )

            # åŠ è½½æƒé‡
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)

            # åŠ è½½æ ‡å‡†åŒ–å™¨
            if 'position_scaler' in checkpoint and 'velocity_scaler' in checkpoint:
                self.position_scaler = checkpoint['position_scaler']
                self.velocity_scaler = checkpoint['velocity_scaler']
                print("VECTORæ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
            else:
                print("è­¦å‘Š: VECTORæ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰æ ‡å‡†åŒ–å™¨")
                self.position_scaler = None
                self.velocity_scaler = None

            self.model.eval()
            self.model_loaded = True
            print("VECTORæ¨¡å‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            print(f"VECTORæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model_loaded = False

    def _compute_velocity(self, pos_history):
        """è®¡ç®—å½“å‰é€Ÿåº¦"""
        if len(pos_history) < 2:
            return np.zeros(3)

        recent_positions = np.array(list(pos_history)[-2:])
        velocity = (recent_positions[-1] - recent_positions[-2]) / self.dt
        return velocity

    def initialize(self, initial_state: np.ndarray) -> bool:
        """åˆå§‹åŒ–é¢„æµ‹å™¨"""
        try:
            initial_pos = initial_state[:3]

            # æ¸…ç©ºç¼“å†²åŒº
            self.velocity_buffer.clear()
            self.position_history.clear()

            # å¡«å……åˆå§‹å†å²
            for _ in range(self.sequence_length):
                self.velocity_buffer.append(np.zeros(3))

            self.position_history.append(initial_pos.copy())

            # é‡ç½®ç»Ÿè®¡
            self.total_predictions = 0
            self.successful_predictions = 0

            return True

        except Exception as e:
            print(f"VECTORé¢„æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def predict_and_update(self, measurement: np.ndarray, **kwargs) -> np.ndarray:
        """å•æ­¥é¢„æµ‹æ–¹æ³•"""
        self.total_predictions += 1
        current_pos = np.array(measurement[:3])

        # æ›´æ–°ä½ç½®å†å²
        self.position_history.append(current_pos.copy())

        # è®¡ç®—å½“å‰é€Ÿåº¦å¹¶æ›´æ–°é€Ÿåº¦ç¼“å†²åŒº
        current_velocity = self._compute_velocity(self.position_history)
        self.velocity_buffer.append(current_velocity.copy())

        if not self.model_loaded or self.model is None:
            # å›é€€åˆ°ç®€å•çš„ç‰©ç†é¢„æµ‹
            return self._physics_fallback(current_pos, current_velocity)

        try:
            # ä½¿ç”¨VECTORæ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                # å‡†å¤‡è¾“å…¥æ•°æ®å¹¶æ ‡å‡†åŒ–
                vel_sequence_raw = np.array(list(self.velocity_buffer))
                current_pos_raw = current_pos.copy()

                # æ ‡å‡†åŒ–è¾“å…¥
                if self.velocity_scaler is not None:
                    vel_sequence_scaled = self.velocity_scaler.transform(vel_sequence_raw)
                else:
                    vel_sequence_scaled = vel_sequence_raw

                if self.position_scaler is not None:
                    current_pos_scaled = self.position_scaler.transform(current_pos_raw.reshape(1, -1)).squeeze()
                else:
                    current_pos_scaled = current_pos_raw

                # è½¬æ¢ä¸ºå¼ é‡
                vel_sequence = torch.FloatTensor(vel_sequence_scaled).unsqueeze(0)  # (1, seq_len, 3)
                last_pos = torch.FloatTensor(current_pos_scaled).unsqueeze(0)  # (1, 3)

                # æ¨¡å‹é¢„æµ‹ï¼ˆåœ¨æ ‡å‡†åŒ–ç©ºé—´ä¸­ï¼‰
                predicted_pos_scaled, predicted_vel_scaled = self.model(vel_sequence, last_pos)

                # è½¬æ¢ä¸ºnumpy
                predicted_pos_scaled = predicted_pos_scaled.squeeze(0).numpy()

                # åæ ‡å‡†åŒ–åˆ°åŸå§‹ç©ºé—´
                if self.position_scaler is not None:
                    predicted_position = self.position_scaler.inverse_transform(
                        predicted_pos_scaled.reshape(1, -1)
                    ).squeeze()
                else:
                    predicted_position = predicted_pos_scaled

                self.successful_predictions += 1

                # è°ƒè¯•ä¿¡æ¯ï¼ˆä»…å‰å‡ æ­¥ï¼‰
                if self.total_predictions <= 3:
                    print(f"VECTORé¢„æµ‹æ­¥éª¤ {self.total_predictions}: æˆåŠŸ")

                return predicted_position

        except Exception as e:
            if self.total_predictions <= 3:
                print(f"VECTORé¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç‰©ç†å›é€€: {e}")

            return self._physics_fallback(current_pos, current_velocity)

    def _physics_fallback(self, current_pos, current_velocity):
        """ç‰©ç†æ¨¡å‹å›é€€æ–¹æ¡ˆ"""
        # é™åˆ¶é€Ÿåº¦
        max_velocity = 25.0  # m/s
        vel_norm = np.linalg.norm(current_velocity)
        if vel_norm > max_velocity:
            current_velocity = current_velocity * (max_velocity / vel_norm)

        # ç®€å•çš„åŒ€é€Ÿé¢„æµ‹
        predicted_pos = current_pos + current_velocity * self.dt
        return predicted_pos


# =========================
# è¯„ä¼°æŒ‡æ ‡
# =========================

def calculate_trajectory_metrics(predictions, targets, threshold=2.0):
    """è®¡ç®—è½¨è¿¹é¢„æµ‹çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡"""
    predictions = np.array(predictions)
    targets = np.array(targets)

    if predictions.ndim == 2:  # å•æ­¥é¢„æµ‹
        errors = np.linalg.norm(predictions - targets, axis=1)
        ade = np.mean(errors)
        fde = ade  # å•æ­¥æƒ…å†µä¸‹ADE=FDE
        success_rate = np.mean(errors < threshold) * 100

    else:  # å¤šæ­¥é¢„æµ‹
        all_errors = np.linalg.norm(predictions - targets, axis=2)  # [N, T]
        ade = np.mean(all_errors)
        final_errors = all_errors[:, -1]  # [N]
        fde = np.mean(final_errors)
        success_rate = np.mean(final_errors < threshold) * 100

    return {
        'ADE': ade,
        'FDE': fde,
        'Success_Rate': success_rate
    }


# =========================
# æ–¹æ³•åˆ›å»ºå‡½æ•°
# =========================

def create_integrated_baseline_methods_complete(qr_model_path=None, vector_model_path=None, dt=0.1, seq_len=20,
                                                device='cpu', **kwargs):
    """åˆ›å»ºé›†æˆçš„7ç§åŸºçº¿æ–¹æ³•ï¼ˆæ ¸å¿ƒ5ç§ + VECTOR + ç‰©ç†æ–¹æ³•ï¼‰"""

    print("=" * 60)
    print("åˆ›å»ºé›†æˆçš„7ç§é«˜è´¨é‡åŸºçº¿æ–¹æ³•å¯¹æ¯”å®éªŒï¼ˆQ/Rç¼©æ”¾ä¸“ç”¨ç‰ˆï¼‰")
    print("=" * 60)
    print(f"å‚æ•°é…ç½®:")
    print(f"  Q/Rç¼©æ”¾æ¨¡å‹: {qr_model_path}")
    print(f"  VECTORæ¨¡å‹: {vector_model_path}")
    print(f"  æ—¶é—´é—´éš”dt: {dt}")
    print(f"  åºåˆ—é•¿åº¦: {seq_len}")
    print(f"  è®¡ç®—è®¾å¤‡: {device}")
    print("=" * 60)

    methods = {}

    # ç¬¬ä¸€éƒ¨åˆ†ï¼šåˆ›å»ºæ ¸å¿ƒ5ç§æ–¹æ³•
    print("\nã€ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒ5ç§åŸºçº¿æ–¹æ³•ã€‘")
    try:
        core_methods = create_baseline_methods(
            qr_model_path=qr_model_path,
            dt=dt,
            seq_len=seq_len,
            device=device,
            **kwargs
        )
        methods.update(core_methods)
        print(f"âœ… æˆåŠŸåˆ›å»º {len(core_methods)} ç§æ ¸å¿ƒæ–¹æ³•: {list(core_methods.keys())}")
    except Exception as e:
        print(f"âŒ æ ¸å¿ƒæ–¹æ³•åˆ›å»ºå¤±è´¥: {e}")

    # ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ›å»ºVECTORæ–¹æ³•
    print("\nã€ç¬¬äºŒéƒ¨åˆ†ï¼šVECTORæ–¹æ³•ã€‘")

    if vector_model_path and os.path.exists(vector_model_path):
        try:
            vector_predictor = VectorPredictor(
                model_path=vector_model_path,
                sequence_length=20,
                dt=dt,
                device=device
            )
            methods['VECTOR'] = MethodWrapper('VECTOR', vector_predictor)

            # æ·»åŠ çŠ¶æ€æ£€æŸ¥
            if vector_predictor.model_loaded:
                print("âœ… VECTORæ–¹æ³•åˆ›å»ºæˆåŠŸï¼Œæ¨¡å‹æ­£ç¡®åŠ è½½")
                print(f"  ä½ç½®æ ‡å‡†åŒ–å™¨: {'å·²åŠ è½½' if vector_predictor.position_scaler is not None else 'æœªåŠ è½½'}")
                print(f"  é€Ÿåº¦æ ‡å‡†åŒ–å™¨: {'å·²åŠ è½½' if vector_predictor.velocity_scaler is not None else 'æœªåŠ è½½'}")
            else:
                print("âš ï¸ VECTORæ–¹æ³•åˆ›å»ºæˆåŠŸï¼Œä½†æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç‰©ç†å›é€€")

        except Exception as e:
            print(f"âŒ åˆ›å»ºVECTORå¤±è´¥: {e}")
    else:
        print(f"âŒ VECTORæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {vector_model_path}")

    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šå°è¯•åˆ›å»ºä¼˜åŒ–çš„Trajectron++ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    print("\nã€ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¼˜åŒ–Trajectron++æ–¹æ³•ã€‘")

    if NEW_METHODS_AVAILABLE:
        try:
            optimized_trajectron = create_optimized_trajectron_plus_plus(
                dt=dt,
                seq_len=30,  # æœ€ä½³å‚æ•°
                device=device,
                hidden_dim=64,  # æœ€ä½³å‚æ•°
                num_layers=2,  # æœ€ä½³å‚æ•°
                dropout=0.1  # æœ€ä½³å‚æ•°
            )

            methods['Trajectron++'] = MethodWrapper('Trajectron++', optimized_trajectron)
            print("âœ… Trajectron++åˆ›å»ºæˆåŠŸ")

        except Exception as e:
            print(f"âŒ åˆ›å»ºä¼˜åŒ–Trajectron++å¤±è´¥: {e}")
    else:
        print("âŒ Trajectron++æ–¹æ³•æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡åˆ›å»º")

    # ç¬¬å››éƒ¨åˆ†ï¼šåˆ›å»ºå¾®åˆ†å¹³å¦æ€§ç‰©ç†é¢„æµ‹å™¨
    print("\nã€ç¬¬å››éƒ¨åˆ†ï¼šå¾®åˆ†å¹³å¦æ€§ç‰©ç†é¢„æµ‹å™¨ã€‘")

    # æ ¹æ®æ•°æ®é›†æ¨æ–­è½½å…·ç±»å‹ï¼ˆåœ¨å®é™…è¯„ä¼°æ—¶ä¼šåŠ¨æ€è®¾ç½®ï¼‰
    try:
        physics_predictor = DifferentialFlatnessTrajectoryPredictor(dt=dt, vehicle_type="medium_large_quad")
        methods['Physics-DF'] = MethodWrapper('Physics-DF', physics_predictor)
        print("âœ… å¾®åˆ†å¹³å¦æ€§ç‰©ç†é¢„æµ‹å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆ›å»ºå¾®åˆ†å¹³å¦æ€§ç‰©ç†é¢„æµ‹å™¨å¤±è´¥: {e}")

    print(f"\næ€»å…±æˆåŠŸåˆ›å»º {len(methods)} ç§æ–¹æ³•:")
    for i, method_name in enumerate(methods.keys(), 1):
        method_type = "ğŸ§ " if any(
            x in method_name for x in ['Q/R', 'VECTOR', 'Transformer', 'Trajectron']) else "âš™ï¸"
        print(f"  {i}. {method_type} {method_name}")

    return methods


# =========================
# æ•°æ®å¤„ç†
# =========================

def extract_sequence_features(pos_window, vel_window, dt):
    """æå–åºåˆ—ç‰¹å¾ - è®­ç»ƒä»£ç åŸç‰ˆï¼ˆ16ç»´ï¼‰"""
    pos_mean = np.mean(pos_window, axis=0)
    pos_std = np.std(pos_window, axis=0)

    vel_mean = np.mean(vel_window, axis=0) if len(vel_window) > 0 else np.zeros(3)
    vel_std = np.std(vel_window, axis=0) if len(vel_window) > 0 else np.zeros(3)
    vel_norm_mean = np.mean(np.linalg.norm(vel_window, axis=1)) if len(vel_window) > 0 else 0.0

    traj_length = np.sum(np.linalg.norm(np.diff(pos_window, axis=0), axis=1))
    displacement = np.linalg.norm(pos_window[-1] - pos_window[0])

    features = np.concatenate([
        pos_mean, pos_std, vel_mean, vel_std,
        [vel_norm_mean, traj_length, displacement, dt]
    ])

    return features


def generate_continuous_samples(positions, dt):
    """ç”Ÿæˆè¿ç»­æ ·æœ¬ - æ›´æ¥è¿‘è®­ç»ƒç¯å¢ƒ"""
    continuous_samples = []
    window_size = 20

    for i in range(window_size, len(positions) - 1):
        pos_window = positions[i - window_size:i]
        vel_window = np.diff(pos_window, axis=0) / dt

        features = extract_sequence_features(pos_window, vel_window, dt)
        target_pos = positions[i + 1]

        continuous_samples.append({
            'features': features,
            'target_pos': target_pos,
            'current_pos': positions[i],
            'dt': dt,
            'step_index': i
        })

    return continuous_samples


def infer_vehicle_type_from_filename(filename):
    """ä»æ–‡ä»¶åæ¨æ–­è½½å…·ç±»å‹"""
    filename_lower = filename.lower()
    if "enhanced_drone_data" in filename_lower or "drone_dt01" in filename_lower:
        return "medium_large_quad"
    elif "drone_flight_data" in filename_lower:
        return "micro_quad"
    elif "fixed_wing" in filename_lower:
        return "fixed_wing"
    elif "heavy_multirotor" in filename_lower:
        return "heavy_multirotor"
    else:
        return "medium_large_quad"


def load_dataset_continuous_style(file_path, max_points=700):
    """æŒ‰è¿ç»­é¢„æµ‹çš„æ–¹å¼åŠ è½½æ•°æ®é›†"""
    print(f"åŠ è½½æ•°æ®é›†: {file_path}")

    try:
        df = pd.read_csv(file_path)

        time_cols = [col for col in df.columns if 'time' in col.lower()]
        if not time_cols:
            print(f"æœªæ‰¾åˆ°æ—¶é—´åˆ—")
            return None

        pos_cols = []
        for prefix in ['x', 'y', 'z']:
            for col in df.columns:
                if col.lower() == prefix or col.lower() == f'{prefix}_true' or col.lower() == f'true_{prefix}':
                    pos_cols.append(col)
                    break

        if len(pos_cols) < 3:
            print(f"ä½ç½®åˆ—ä¸å®Œæ•´: {pos_cols}")
            return None

        positions_full = df[pos_cols].values[:, :3]
        print(f"  åŸå§‹æ•°æ®: {len(positions_full)} å¸§")

        positions = positions_full[:max_points]
        print(f"  é™åˆ¶åæ•°æ®: {len(positions)} å¸§ (å‰{max_points}å¸§)")

        if len(positions) < 50:
            print(f"æ•°æ®ç‚¹å¤ªå°‘: {len(positions)}")
            return None

        if np.any(np.isnan(positions)):
            print(f"  å¤„ç†NaNå€¼...")
            valid_mask = ~np.any(np.isnan(positions), axis=1)
            positions = positions[valid_mask]

        if len(positions) < 50:
            print(f"æ¸…ç†åæ•°æ®ç‚¹å¤ªå°‘: {len(positions)}")
            return None

        time_data = df[time_cols[0]].values[:len(positions)]
        if len(time_data) > 1:
            dt_values = np.diff(time_data)
            dt = np.median(dt_values)
        else:
            dt = 0.1

        print(f"  æ—¶é—´é—´éš”: {dt:.3f}s")

        # ä½¿ç”¨ç›¸åŒçš„80/20åˆ’åˆ†
        split_point = int(len(positions) * 0.8)
        train_positions = positions[:split_point]
        test_positions = positions[split_point:]

        print(f"  è®­ç»ƒæ•°æ®: {len(train_positions)} å¸§")
        print(f"  æµ‹è¯•æ•°æ®: {len(test_positions)} å¸§")

        # ç”Ÿæˆè¿ç»­æ ·æœ¬
        test_samples = generate_continuous_samples(test_positions, dt)
        print(f"  è¿ç»­æµ‹è¯•æ ·æœ¬: {len(test_samples)} ä¸ª")

        # æ¨æ–­è½½å…·ç±»å‹
        vehicle_type = infer_vehicle_type_from_filename(os.path.basename(file_path))
        print(f"  è½½å…·ç±»å‹: {vehicle_type}")

        return {
            'test_samples': test_samples,
            'test_positions': test_positions,
            'dt': dt,
            'filename': os.path.basename(file_path),
            'vehicle_type': vehicle_type,
            'split_info': {
                'total_frames': len(positions),
                'train_frames': len(train_positions),
                'test_frames': len(test_positions),
                'test_samples': len(test_samples)
            }
        }

    except Exception as e:
        print(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return None


# =========================
# è¯„ä¼°å™¨
# =========================

class ImprovedBaselineMethodsEvaluator:
    """æ”¹è¿›çš„åŸºçº¿æ–¹æ³•è¯„ä¼°å™¨ - æ”¯æŒè½½å…·ç±»å‹åŠ¨æ€é…ç½®"""

    def __init__(self, qr_model_path=None, vector_model_path=None, device='cpu'):
        self.qr_model_path = qr_model_path
        self.vector_model_path = vector_model_path
        self.device = device
        self.methods = {}
        self.method_states = {}

    def initialize_methods(self, dt=0.1, vehicle_type="medium_large_quad"):
        """åˆå§‹åŒ–æ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼ˆ7ç§ï¼‰"""
        print(f"åˆå§‹åŒ–7ç§é›†æˆåŸºçº¿æ–¹æ³•...")
        print(f"è½½å…·ç±»å‹: {vehicle_type}")

        # ä½¿ç”¨æ–°çš„é›†æˆæ–¹æ³•åˆ›å»ºå‡½æ•°
        self.methods = create_integrated_baseline_methods_complete(
            qr_model_path=self.qr_model_path,
            vector_model_path=self.vector_model_path,
            dt=dt,
            device=self.device
        )

        # ä¸ºå¾®åˆ†å¹³å¦æ€§ç‰©ç†é¢„æµ‹å™¨é‡æ–°è®¾ç½®è½½å…·ç±»å‹
        if 'Physics-DF' in self.methods:
            try:
                physics_predictor = DifferentialFlatnessTrajectoryPredictor(dt=dt, vehicle_type=vehicle_type)
                self.methods['Physics-DF'] = MethodWrapper('Physics-DF', physics_predictor)
                print(f"âœ… å¾®åˆ†å¹³å¦æ€§ç‰©ç†é¢„æµ‹å™¨å·²é…ç½®ä¸º {vehicle_type}")
            except Exception as e:
                print(f"âŒ é‡æ–°é…ç½®ç‰©ç†é¢„æµ‹å™¨å¤±è´¥: {e}")

        print(f"æˆåŠŸåˆå§‹åŒ– {len(self.methods)} ç§æ–¹æ³•: {list(self.methods.keys())}")

        # æ£€æŸ¥Q/Rç¼©æ”¾UKFçš„çŠ¶æ€
        if 'Q/Rç¼©æ”¾UKF' in self.methods:
            self._check_qr_scaling_model_status()

        # æ£€æŸ¥VECTORçš„çŠ¶æ€
        if 'VECTOR' in self.methods:
            self._check_vector_model_status()

        return len(self.methods) > 0

    def _check_qr_scaling_model_status(self):
        """æ£€æŸ¥Q/Rç¼©æ”¾æ¨¡å‹çš„åŠ è½½çŠ¶æ€"""
        print("\n=== Q/Rç¼©æ”¾æ¨¡å‹çŠ¶æ€æ£€æŸ¥ ===")
        try:
            qr_method = self.methods['Q/Rç¼©æ”¾UKF'].method

            print(f"æ¨¡å‹åŠ è½½çŠ¶æ€: {qr_method.model_loaded}")

            if hasattr(qr_method, 'qr_model') and qr_method.qr_model is not None:
                print("âœ“ Q/Rç¼©æ”¾ç¥ç»ç½‘ç»œæ¨¡å‹å·²åŠ è½½")
                test_input = torch.randn(1, 16)
                with torch.no_grad():
                    qr_scales, confidence, vehicle_probs, vehicle_logits = qr_method.qr_model(test_input)
                print(f"æ¨¡å‹æµ‹è¯•è¾“å‡º: Q/Rç¼©æ”¾={qr_scales.flatten().numpy()}")
            else:
                print("âœ— Q/Rç¼©æ”¾ç¥ç»ç½‘ç»œæ¨¡å‹æœªåŠ è½½")

            print("=== çŠ¶æ€æ£€æŸ¥å®Œæˆ ===\n")

        except Exception as e:
            print(f"çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")

    def _check_vector_model_status(self):
        """æ£€æŸ¥VECTORæ¨¡å‹çš„åŠ è½½çŠ¶æ€"""
        print("\n=== VECTORæ¨¡å‹çŠ¶æ€æ£€æŸ¥ ===")
        try:
            vector_method = self.methods['VECTOR'].method

            print(f"æ¨¡å‹åŠ è½½çŠ¶æ€: {vector_method.model_loaded}")

            if vector_method.model_loaded:
                print("âœ“ VECTORç¥ç»ç½‘ç»œæ¨¡å‹å·²åŠ è½½")
                print(f"  ä½ç½®æ ‡å‡†åŒ–å™¨: {'å·²åŠ è½½' if vector_method.position_scaler is not None else 'æœªåŠ è½½'}")
                print(f"  é€Ÿåº¦æ ‡å‡†åŒ–å™¨: {'å·²åŠ è½½' if vector_method.velocity_scaler is not None else 'æœªåŠ è½½'}")

                # æµ‹è¯•æ¨¡å‹æ¨ç†
                try:
                    test_vel_seq = torch.randn(1, 20, 3)
                    test_pos = torch.randn(1, 3)
                    with torch.no_grad():
                        pred_pos, pred_vel = vector_method.model(test_vel_seq, test_pos)
                    print(f"  æ¨¡å‹æ¨ç†æµ‹è¯•: æˆåŠŸ")
                except Exception as e:
                    print(f"  æ¨¡å‹æ¨ç†æµ‹è¯•: å¤±è´¥ - {e}")
            else:
                print("âœ— VECTORç¥ç»ç½‘ç»œæ¨¡å‹æœªåŠ è½½ï¼Œå°†ä½¿ç”¨ç‰©ç†å›é€€")

            print("=== çŠ¶æ€æ£€æŸ¥å®Œæˆ ===\n")

        except Exception as e:
            print(f"çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")

    def _evaluate_single_step_continuous_with_metrics(self, sample, method_wrapper, verbose=False):
        """å•æ­¥è¿ç»­è¯„ä¼° - å¢åŠ æ ‡å‡†è½¨è¿¹é¢„æµ‹æŒ‡æ ‡å’Œç²¾ç¡®æ—¶é—´æµ‹é‡"""
        try:
            current_pos = sample['current_pos']
            target_pos = sample['target_pos']

            # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ—¶é—´æµ‹é‡
            step_start_time = time.perf_counter()

            # 3æ­¥è¿ç»­é¢„æµ‹è¿‡ç¨‹
            max_steps = 3
            current_pred = current_pos.copy()
            trajectory_predictions = [current_pos.copy()]  # è®°å½•é¢„æµ‹è½¨è¿¹
            trajectory_targets = [current_pos.copy()]  # è®°å½•ç›®æ ‡è½¨è¿¹

            for step in range(max_steps):
                progress = (step + 1) / max_steps
                intermediate_target = current_pos + progress * (target_pos - current_pos)
                trajectory_targets.append(intermediate_target.copy())

                # æ·»åŠ 0.25må™ªå£°
                noise = np.random.normal(0, 0.25, 3)
                noisy_obs = intermediate_target + noise

                pred_result = method_wrapper.predict_and_update(noisy_obs)
                current_pred = pred_result[:3] if len(pred_result) > 3 else pred_result
                trajectory_predictions.append(current_pred.copy())

            # è®¡ç®—å•æ­¥æ—¶é—´
            step_time = (time.perf_counter() - step_start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

            # è®¡ç®—æ ‡å‡†è½¨è¿¹é¢„æµ‹æŒ‡æ ‡
            pred_array = np.array(trajectory_predictions[1:])  # å»æ‰åˆå§‹ä½ç½®
            target_array = np.array(trajectory_targets[1:])  # å»æ‰åˆå§‹ä½ç½®

            metrics = calculate_trajectory_metrics(
                pred_array.reshape(1, -1, 3),
                target_array.reshape(1, -1, 3)
            )

            # æ·»åŠ æ—¶é—´ä¿¡æ¯
            metrics['Processing_Time'] = step_time

            if verbose:
                print(f"    ADE: {metrics['ADE']:.6f}m")
                print(f"    FDE: {metrics['FDE']:.6f}m")
                print(f"    Time: {step_time:.3f}ms")
                print(f"    Success: {'Yes' if metrics['FDE'] < 2.0 else 'No'}")

            return metrics

        except Exception as e:
            if verbose:
                print(f"    é¢„æµ‹å¤±è´¥: {e}")
            return {'ADE': float('inf'), 'FDE': float('inf'), 'Success_Rate': 0.0, 'Processing_Time': float('inf')}

    def evaluate_continuous_trajectory_with_metrics(self, dataset, max_samples_per_method=100):
        """è¿ç»­è½¨è¿¹è¯„ä¼° - è¾“å‡ºæ ‡å‡†è½¨è¿¹é¢„æµ‹æŒ‡æ ‡å’Œç²¾ç¡®æ—¶é—´ç»Ÿè®¡"""
        print(f"\nè¿ç»­è½¨è¿¹è¯„ä¼°: {dataset['filename']}")
        print(f"æ•°æ®ä¿¡æ¯: {dataset['split_info']}")
        print(f"è½½å…·ç±»å‹: {dataset['vehicle_type']}")

        test_samples = dataset['test_samples']
        test_positions = dataset['test_positions']

        if max_samples_per_method and max_samples_per_method > 0:
            eval_samples = min(len(test_samples), max_samples_per_method)
            test_samples = test_samples[:eval_samples]
            print(f"è¯„ä¼° {eval_samples} ä¸ªè¿ç»­æ ·æœ¬ (é™åˆ¶ä¸º{max_samples_per_method})...")
        else:
            eval_samples = len(test_samples)
            print(f"è¯„ä¼°å…¨éƒ¨ {eval_samples} ä¸ªè¿ç»­æ ·æœ¬...")

        results = {}

        for method_name, method_wrapper in self.methods.items():
            print(f"\n--- è¿ç»­è¯„ä¼° {method_name} ---")

            # åˆå§‹åŒ–
            initial_pos = test_positions[20]
            initial_state = np.concatenate([initial_pos, [0, 0, 0]])

            if not method_wrapper.initialize(initial_state):
                print(f"  {method_name} åˆå§‹åŒ–å¤±è´¥")
                continue

            print(f"  {method_name} åˆå§‹åŒ–æˆåŠŸ")

            # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„æŒ‡æ ‡
            all_ade = []
            all_fde = []
            all_success = []
            all_processing_times = []

            for i, sample in enumerate(test_samples):
                np.random.seed(42 + i)  # ç¡®ä¿å¯é‡å¤æ€§

                metrics = self._evaluate_single_step_continuous_with_metrics(
                    sample, method_wrapper, verbose=(i < 3)
                )

                if metrics['ADE'] != float('inf'):
                    all_ade.append(metrics['ADE'])
                    all_fde.append(metrics['FDE'])
                    all_success.append(1 if metrics['FDE'] < 2.0 else 0)
                    all_processing_times.append(metrics['Processing_Time'])

            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            if len(all_ade) > 0:
                avg_ade = np.mean(all_ade)
                avg_fde = np.mean(all_fde)
                success_rate = np.mean(all_success) * 100
                avg_processing_time = np.mean(all_processing_times)

                results[method_name] = {
                    'ADE': avg_ade,
                    'FDE': avg_fde,
                    'Success_Rate': success_rate,
                    'Processing_Time': avg_processing_time,
                    'samples_evaluated': len(all_ade),
                    'total_samples': len(test_samples)
                }

                print(
                    f"  {method_name}: ADE={avg_ade:.6f}m, FDE={avg_fde:.6f}m, æˆåŠŸç‡={success_rate:.1f}%, å¤„ç†æ—¶é—´={avg_processing_time:.3f}ms")
            else:
                print(f"  {method_name}: æ‰€æœ‰é¢„æµ‹éƒ½å¤±è´¥äº†!")
                results[method_name] = {
                    'ADE': float('inf'), 'FDE': float('inf'), 'Success_Rate': 0.0,
                    'Processing_Time': float('inf'),
                    'samples_evaluated': 0, 'total_samples': len(test_samples)
                }

        return results


def print_final_results_with_metrics(all_results):
    """è¾“å‡ºæ ‡å‡†è½¨è¿¹é¢„æµ‹æŒ‡æ ‡çš„æœ€ç»ˆç»“æœ"""
    print(f"\n{'=' * 60}")
    print("è½¨è¿¹é¢„æµ‹æ–¹æ³•å¯¹æ¯”ç»“æœï¼ˆæ ‡å‡†æŒ‡æ ‡ï¼‰")
    print("=" * 60)

    # æ±‡æ€»æ‰€æœ‰æ–¹æ³•çš„å¹³å‡æŒ‡æ ‡
    method_metrics = {}
    for result in all_results:
        for method_name, metrics in result.items():
            if 'dataset_info' in method_name:
                continue

            if method_name not in method_metrics:
                method_metrics[method_name] = {
                    'ADE': [], 'FDE': [], 'Success_Rate': [], 'Processing_Time': []
                }

            if metrics['ADE'] != float('inf'):
                method_metrics[method_name]['ADE'].append(metrics['ADE'])
                method_metrics[method_name]['FDE'].append(metrics['FDE'])
                method_metrics[method_name]['Success_Rate'].append(metrics['Success_Rate'])
                method_metrics[method_name]['Processing_Time'].append(metrics['Processing_Time'])

    # è®¡ç®—å¹³å‡å€¼å¹¶æ’åº
    final_results = []
    for method_name, metrics in method_metrics.items():
        if len(metrics['ADE']) > 0:
            avg_ade = np.mean(metrics['ADE'])
            avg_fde = np.mean(metrics['FDE'])
            avg_success = np.mean(metrics['Success_Rate'])
            avg_time = np.mean(metrics['Processing_Time'])
            final_results.append((method_name, avg_ade, avg_fde, avg_success, avg_time))

    # æŒ‰ADEæ’åº
    final_results.sort(key=lambda x: x[1])

    print("\næ€»ä½“æ€§èƒ½æ’å:")
    for rank, (method_name, ade, fde, success_rate, proc_time) in enumerate(final_results, 1):
        method_type = "ğŸ§ " if any(
            x in method_name for x in ['Q/R', 'VECTOR', 'Transformer', 'Trajectron']) else "âš™ï¸"
        print(
            f"  #{rank} {method_type} {method_name}: ADE={ade:.6f}m, FDE={fde:.6f}m, æˆåŠŸç‡={success_rate:.1f}%, å¤„ç†æ—¶é—´={proc_time:.3f}ms")

    # åˆ†ç±»åˆ†æ
    print(f"\næ–¹æ³•åˆ†ç±»åˆ†æ:")
    neural_methods = [r for r in final_results if
                      any(x in r[0] for x in ['Q/R', 'VECTOR', 'Transformer', 'Trajectron'])]
    physics_methods = [r for r in final_results if any(x in r[0] for x in ['UKF', 'Kalman', 'Physics', 'ç‰©ç†'])]

    if neural_methods:
        best_neural = min(neural_methods, key=lambda x: x[1])
        print(f"  æœ€ä½³ç¥ç»ç½‘ç»œæ–¹æ³•: {best_neural[0]} (ADE={best_neural[1]:.6f}m)")

    if physics_methods:
        best_physics = min(physics_methods, key=lambda x: x[1])
        print(f"  æœ€ä½³ç‰©ç†æ–¹æ³•: {best_physics[0]} (ADE={best_physics[1]:.6f}m)")

    # è®¡ç®—æ•ˆç‡åˆ†æ
    print(f"\nè®¡ç®—æ•ˆç‡åˆ†æ:")
    fast_methods = [r for r in final_results if r[4] < 1.0]  # <1msçš„æ–¹æ³•
    medium_methods = [r for r in final_results if 1.0 <= r[4] < 10.0]  # 1-10msçš„æ–¹æ³•
    slow_methods = [r for r in final_results if r[4] >= 10.0]  # >10msçš„æ–¹æ³•

    print(f"  å®æ—¶æ–¹æ³• (<1ms): {len(fast_methods)} ç§")
    print(f"  å‡†å®æ—¶æ–¹æ³• (1-10ms): {len(medium_methods)} ç§")
    print(f"  æ…¢é€Ÿæ–¹æ³• (>10ms): {len(slow_methods)} ç§")

    # è¾“å‡ºè®ºæ–‡æ ¼å¼ç»“æœ
    print(f"\nè®ºæ–‡æ ¼å¼ç»“æœ:")
    for rank, (method_name, ade, fde, success_rate, proc_time) in enumerate(final_results, 1):
        print(f"{method_name}: ADE={ade:.6f}m, FDE={fde:.6f}m, æˆåŠŸç‡={success_rate:.1f}%, æ—¶é—´={proc_time:.3f}ms")

    return final_results


def main():
    """ä¿®æ”¹åçš„ä¸»å‡½æ•° - æ”¯æŒå¤šæ¬¡è¿è¡Œ"""
    parser = argparse.ArgumentParser(description='é›†æˆ7ç§åŸºçº¿æ–¹æ³•å¯¹æ¯”å®éªŒ - å¤šæ¬¡è¿è¡Œç»Ÿè®¡ç‰ˆ')

    # ç°æœ‰å‚æ•°...
    parser.add_argument('--qr_model', type=str, default='BEST_qr_scaling.pth',
                        help='Q/Rç¼©æ”¾æ¨¡å‹è·¯å¾„ (default: BEST_qr_scaling.pth)')
    parser.add_argument('--vector_model', type=str, default='vector_best_model.pth', help='VECTORæ¨¡å‹è·¯å¾„')
    parser.add_argument('--device', type=str, default='cpu', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--max_points', type=int, default=700, help='æ¯æ•°æ®é›†æœ€å¤§å¸§æ•°')
    parser.add_argument('--data_files', type=str, help='æ•°æ®æ–‡ä»¶åˆ—è¡¨ï¼Œé€—å·åˆ†éš”')
    parser.add_argument('--max_samples', type=int, default=100, help='æ¯æ–¹æ³•æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°')

    # æ–°å¢å‚æ•°
    parser.add_argument('--num_runs', type=int, default=5, help='ç‹¬ç«‹è¿è¡Œæ¬¡æ•° (default: 5)')
    parser.add_argument('--random_seed', type=int, default=42, help='éšæœºç§å­åŸºå€¼ (default: 42)')

    args = parser.parse_args()

    print("é›†æˆ7ç§é«˜è´¨é‡åŸºçº¿æ–¹æ³•å¯¹æ¯”å®éªŒï¼ˆå¤šæ¬¡è¿è¡Œç»Ÿè®¡ç‰ˆï¼‰")
    print("=" * 60)
    print("å®éªŒè®¾ç½®:")
    print(f"- ç‹¬ç«‹è¿è¡Œæ¬¡æ•°: {args.num_runs}")
    print(f"- éšæœºç§å­åŸºå€¼: {args.random_seed}")
    print(f"- æ¯æ¬¡è¿è¡Œå°†ä½¿ç”¨ç§å­: {[args.random_seed + i for i in range(args.num_runs)]}")
    print("=" * 60)

    # å‡†å¤‡æ•°æ®é›†ï¼ˆåªéœ€è¦åŠ è½½ä¸€æ¬¡ï¼‰
    if args.data_files:
        data_files = [f.strip() for f in args.data_files.split(',') if f.strip()]
    else:
        data_files = [
            "drone_flight_data.csv",
            "drone_dt01.csv",
            "complex_fixed_wing_trajectory.csv",
            "complex_heavy_multirotor_trajectory.csv"
        ]

    print(f"\nå‡†å¤‡æ•°æ®é›†...")
    datasets = []
    for data_file in data_files:
        if os.path.exists(data_file):
            dataset = load_dataset_continuous_style(data_file, args.max_points)
            if dataset is not None:
                datasets.append(dataset)
        else:
            print(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")

    if len(datasets) == 0:
        print("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†")
        return

    print(f"æˆåŠŸåŠ è½½ {len(datasets)} ä¸ªæ•°æ®é›†")

    # å¤šæ¬¡è¿è¡Œå®éªŒ
    all_runs_results = []

    for run_idx in range(args.num_runs):
        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹ç¬¬ {run_idx + 1}/{args.num_runs} æ¬¡è¿è¡Œ")
        print("=" * 60)

        # è®¾ç½®å½“å‰è¿è¡Œçš„éšæœºç§å­
        current_seed = args.random_seed + run_idx
        np.random.seed(current_seed)
        torch.manual_seed(current_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(current_seed)

        print(f"å½“å‰è¿è¡Œéšæœºç§å­: {current_seed}")

        # åˆ›å»ºè¯„ä¼°å™¨ï¼ˆæ¯æ¬¡è¿è¡Œéƒ½é‡æ–°åˆ›å»ºï¼‰
        evaluator = ImprovedBaselineMethodsEvaluator(
            qr_model_path=args.qr_model,
            vector_model_path=args.vector_model,
            device=args.device
        )

        # å½“å‰è¿è¡Œçš„ç»“æœ
        current_run_results = []

        # å¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œè¯„ä¼°
        for dataset_idx, dataset in enumerate(datasets):
            print(f"\n--- Run {run_idx + 1}: æ•°æ®é›† {dataset_idx + 1}/{len(datasets)} ---")

            # æ ¹æ®æ•°æ®é›†è½½å…·ç±»å‹åˆå§‹åŒ–æ–¹æ³•
            vehicle_type = dataset['vehicle_type']
            if not evaluator.initialize_methods(dt=dataset['dt'], vehicle_type=vehicle_type):
                print(f"Run {run_idx + 1}: æ–¹æ³•åˆå§‹åŒ–å¤±è´¥")
                continue

            # è¯„ä¼°å½“å‰æ•°æ®é›†
            dataset_results = evaluator.evaluate_continuous_trajectory_with_metrics(
                dataset, args.max_samples
            )

            if dataset_results:
                dataset_results['dataset_info'] = {
                    'filename': dataset['filename'],
                    'vehicle_type': dataset['vehicle_type'],
                    'dt': dataset['dt'],
                    'run_index': run_idx
                }
                current_run_results.append(dataset_results)

        # ä¿å­˜å½“å‰è¿è¡Œçš„ç»“æœ
        all_runs_results.append(current_run_results)
        print(f"ç¬¬ {run_idx + 1} æ¬¡è¿è¡Œå®Œæˆ")

    # ç»Ÿè®¡åˆ†æå’Œç»“æœè¾“å‡º
    if len(all_runs_results) > 0:
        print(f"\n{'=' * 60}")
        print(f"æ‰€æœ‰ {args.num_runs} æ¬¡è¿è¡Œå®Œæˆï¼Œå¼€å§‹ç»Ÿè®¡åˆ†æ...")
        print("=" * 60)

        final_stats = print_final_results_with_stats(all_runs_results, args.num_runs)

        # è¾“å‡ºå®éªŒæ€»ç»“
        print(f"\n{'=' * 60}")
        print("å®éªŒæ€»ç»“:")
        print("=" * 60)
        print(f"âœ… æˆåŠŸå®Œæˆ {args.num_runs} æ¬¡ç‹¬ç«‹è¿è¡Œ")
        print(f"âœ… è¯„ä¼°äº† {len(evaluator.methods) if 'evaluator' in locals() else 'N/A'} ç§æ–¹æ³•")
        print(f"âœ… æµ‹è¯•äº† {len(datasets)} ä¸ªæ•°æ®é›†")
        print(f"âœ… é‡‡ç”¨å‡å€¼Â±æ ‡å‡†å·®æŠ¥å‘Šç»“æœï¼Œç¬¦åˆå­¦æœ¯æ ‡å‡†")
        print(f"âœ… éšæœºç§å­: {args.random_seed} ~ {args.random_seed + args.num_runs - 1}")

        # æœ€ä½³æ–¹æ³•ç»Ÿè®¡
        if final_stats:
            best_method = final_stats[0]
            print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method[0]}")
            print(f"   ADE: {best_method[1]:.6f}Â±{best_method[2]:.6f}m")
            print(f"   FDE: {best_method[3]:.6f}Â±{best_method[4]:.6f}m")
    else:
        print("æ²¡æœ‰è·å¾—æœ‰æ•ˆçš„å®éªŒç»“æœ")


if __name__ == "__main__":
    main()